{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cb75397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 다운로드 성공: ChatbotData.csv\n",
      "데이터 로드 성공!\n",
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "def download_chatbot_data(url, save_path):\n",
    "    \"\"\"\n",
    "    챗봇 데이터를 GitHub에서 다운로드하여 로컬에 저장하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        url (str): 데이터 다운로드 URL.\n",
    "        save_path (str): 저장할 로컬 파일 경로.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        print(f\"데이터 다운로드 성공: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 다운로드 실패: {e}\")\n",
    "\n",
    "def load_chatbot_data(file_path):\n",
    "    \"\"\"\n",
    "    송영숙님의 챗봇 데이터를 로드하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 데이터 파일 경로.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: 로드된 데이터프레임.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"데이터 로드 성공!\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 데이터 다운로드 URL 및 저장 경로\n",
    "data_url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "local_file_path = \"ChatbotData.csv\"\n",
    "\n",
    "# 데이터 다운로드\n",
    "download_chatbot_data(data_url, local_file_path)\n",
    "\n",
    "# 데이터 로드\n",
    "chatbot_data = load_chatbot_data(local_file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "if chatbot_data is not None:\n",
    "    print(chatbot_data.head())  # 상위 5개 데이터 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d9d7315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 11823행, 3열\n"
     ]
    }
   ],
   "source": [
    "data_info = chatbot_data.shape  # (행 개수, 열 개수)\n",
    "print(f\"전체 데이터 크기: {data_info[0]}행, {data_info[1]}열\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab3f6493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q           A  label\n",
      "0            12시 땡   하루가 또 가네요      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "4              심하네   눈살이 찌푸려지죠      0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_chatbot_data(data):\n",
    "    \"\"\"\n",
    "    챗봇 데이터를 전처리하는 함수.\n",
    "    - 특수문자 제거 (숫자와 한글, 공백은 유지)\n",
    "    - 텍스트 정규화\n",
    "    - 중복 및 결측치 제거\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): 원본 데이터프레임\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: 전처리된 데이터프레임\n",
    "    \"\"\"\n",
    "    # 특수문자 제거 (숫자, 한글, 공백만 남김)\n",
    "    data['Q'] = data['Q'].apply(lambda x: re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ0-9\\s]', '', x).strip())\n",
    "    data['A'] = data['A'].apply(lambda x: re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ0-9\\s]', '', x).strip())\n",
    "\n",
    "    # 중복 제거\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # 결측치 제거\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "# 데이터 전처리 실행\n",
    "preprocessed_data = preprocess_chatbot_data(chatbot_data)\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print(preprocessed_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7823cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 데이터 토큰화 개수: 11820\n",
      "A 데이터 토큰화 개수: 11820\n",
      "Q 데이터 예시: [[8354, 8393, 8596, 8510, 8522], [8339, 8393, 504, 642]]\n",
      "A 데이터 예시: [[4095, 162, 2838], [760, 2618]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# SubwordTextEncoder를 사용하여 데이터 토크나이징\n",
    "def tokenize_with_subword(data, column):\n",
    "    \"\"\"\n",
    "    SubwordTextEncoder를 사용하여 데이터를 토크나이징하는 함수.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): 전처리된 데이터프레임\n",
    "        column (str): 토크나이징할 열 이름 (예: 'Q' 또는 'A')\n",
    "\n",
    "    Returns:\n",
    "        encoder, tokenized_data: SubwordTextEncoder 객체와 토크나이징된 데이터 리스트\n",
    "    \"\"\"\n",
    "    # 모든 문장을 리스트로 변환\n",
    "    sentences = data[column].tolist()\n",
    "\n",
    "    # SubwordTextEncoder 학습\n",
    "    encoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "        sentences, target_vocab_size=2**13\n",
    "    )\n",
    "\n",
    "    # 데이터를 토큰화\n",
    "    tokenized_data = [encoder.encode(sentence) for sentence in sentences]\n",
    "\n",
    "    return encoder, tokenized_data\n",
    "\n",
    "# Q와 A 열 각각에 대해 토크나이징 수행\n",
    "q_encoder, q_tokenized = tokenize_with_subword(preprocessed_data, 'Q')\n",
    "a_encoder, a_tokenized = tokenize_with_subword(preprocessed_data, 'A')\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Q 데이터 토큰화 개수: {len(q_tokenized)}\")\n",
    "print(f\"A 데이터 토큰화 개수: {len(a_tokenized)}\")\n",
    "print(f\"Q 데이터 예시: {q_tokenized[:2]}\")\n",
    "print(f\"A 데이터 예시: {a_tokenized[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 트랜스포머 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf285d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embedding (None, 40, 128)           1108096   \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 40, 128)           396032    \n",
      "_________________________________________________________________\n",
      "transformer_block_1 (Transfo (None, 40, 128)           396032    \n",
      "_________________________________________________________________\n",
      "transformer_block_2 (Transfo (None, 40, 128)           396032    \n",
      "_________________________________________________________________\n",
      "transformer_block_3 (Transfo (None, 40, 128)           396032    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 40, 8617)          1111593   \n",
      "=================================================================\n",
      "Total params: 3,803,817\n",
      "Trainable params: 3,803,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 트랜스포머 블록 클래스 정의\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# 토큰 및 포지션 임베딩 클래스 정의\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# 트랜스포머 모델 구성 함수\n",
    "def build_transformer_model(vocab_size, maxlen, embed_dim=128, num_heads=4, ff_dim=512, num_blocks=4):\n",
    "    \"\"\"\n",
    "    트랜스포머 기반 모델을 구성하는 함수.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): 어휘 크기\n",
    "        maxlen (int): 입력 시퀀스 최대 길이\n",
    "        embed_dim (int): 임베딩 차원 크기\n",
    "        num_heads (int): 멀티헤드 어텐션의 헤드 수\n",
    "        ff_dim (int): 피드포워드 네트워크의 차원 크기\n",
    "        num_blocks (int): 트랜스포머 블록 수\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Model: 트랜스포머 모델 객체\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 하이퍼파라미터 설정 및 모델 생성\n",
    "vocab_size = q_encoder.vocab_size  # SubwordTextEncoder에서 생성된 어휘 크기 사용\n",
    "maxlen = 40  # 입력 시퀀스 최대 길이 설정\n",
    "transformer_model = build_transformer_model(vocab_size=vocab_size, maxlen=maxlen)\n",
    "\n",
    "# 모델 요약 출력\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1545c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32ac1a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 질문 개수: 9456\n",
      "테스트 데이터 질문 개수: 2364\n",
      "훈련 데이터 답변 개수: 9456\n",
      "테스트 데이터 답변 개수: 2364\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 준비 (Step 3에서 생성된 토큰화된 데이터 사용)\n",
    "q_data = pad_sequences(q_tokenized, maxlen=maxlen, padding='post')  # 질문 데이터 패딩\n",
    "a_data = pad_sequences(a_tokenized, maxlen=maxlen, padding='post')  # 답변 데이터 패딩\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분리\n",
    "q_train, q_test, a_train, a_test = train_test_split(q_data, a_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 준비 결과 출력\n",
    "print(f\"훈련 데이터 질문 개수: {len(q_train)}\")\n",
    "print(f\"테스트 데이터 질문 개수: {len(q_test)}\")\n",
    "print(f\"훈련 데이터 답변 개수: {len(a_train)}\")\n",
    "print(f\"테스트 데이터 답변 개수: {len(a_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d495ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "148/148 [==============================] - 13s 51ms/step - loss: 1.8867 - accuracy: 0.8892 - val_loss: 0.9662 - val_accuracy: 0.8954\n",
      "Epoch 2/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.9332 - accuracy: 0.8957 - val_loss: 0.9365 - val_accuracy: 0.8957\n",
      "Epoch 3/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.8945 - accuracy: 0.8958 - val_loss: 0.9367 - val_accuracy: 0.8952\n",
      "Epoch 4/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.8565 - accuracy: 0.8960 - val_loss: 0.9457 - val_accuracy: 0.8951\n",
      "Epoch 5/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.8122 - accuracy: 0.8964 - val_loss: 0.9644 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.7612 - accuracy: 0.8978 - val_loss: 0.9909 - val_accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.7002 - accuracy: 0.9011 - val_loss: 1.0006 - val_accuracy: 0.8931\n",
      "Epoch 8/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.6272 - accuracy: 0.9069 - val_loss: 1.0382 - val_accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.5478 - accuracy: 0.9147 - val_loss: 1.0593 - val_accuracy: 0.8864\n",
      "Epoch 10/10\n",
      "148/148 [==============================] - 7s 47ms/step - loss: 0.4671 - accuracy: 0.9233 - val_loss: 1.0855 - val_accuracy: 0.8894\n",
      "모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일\n",
    "transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = transformer_model.fit(\n",
    "    q_train,\n",
    "    a_train,\n",
    "    validation_data=(q_test, a_test),\n",
    "    batch_size=64,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 학습 결과 출력\n",
    "print(\"모델 학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b7e2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 19ms/step - loss: 1.0855 - accuracy: 0.8894\n",
      "테스트 손실: 1.0854589939117432\n",
      "테스트 정확도: 0.8894141316413879\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터로 모델 평가\n",
    "test_loss, test_accuracy = transformer_model.evaluate(q_test, a_test)\n",
    "print(f\"테스트 손실: {test_loss}\")\n",
    "print(f\"테스트 정확도: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5-2: 한국어 입력 문장에 대한 답변 생성 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cac8e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(input_sentence, model, q_encoder, a_encoder, maxlen):\n",
    "    \"\"\"\n",
    "    입력 문장에 대해 모델이 예측한 답변을 생성하는 함수.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): 사용자 입력 문장\n",
    "        model (tensorflow.keras.Model): 학습된 트랜스포머 모델\n",
    "        q_encoder (SubwordTextEncoder): 질문 토크나이저\n",
    "        a_encoder (SubwordTextEncoder): 답변 토크나이저\n",
    "        maxlen (int): 입력 시퀀스 최대 길이\n",
    "\n",
    "    Returns:\n",
    "        str: 모델이 생성한 답변\n",
    "    \"\"\"\n",
    "    # 입력 문장을 토큰화\n",
    "    input_tokens = q_encoder.encode(input_sentence)\n",
    "    input_tokens = pad_sequences([input_tokens], maxlen=maxlen, padding='post')\n",
    "\n",
    "    # 모델 예측 수행\n",
    "    predictions = model.predict(input_tokens)\n",
    "\n",
    "    # 가장 높은 확률의 토큰 선택\n",
    "    predicted_tokens = np.argmax(predictions[0], axis=-1)\n",
    "\n",
    "    # 종료 토큰(<end>) 이후의 토큰 제거 (수동 설정 필요 시 사용)\n",
    "    if 0 in predicted_tokens:  # 0은 일반적으로 패딩 토큰으로 사용됨\n",
    "        predicted_tokens = predicted_tokens[:predicted_tokens.tolist().index(0)]\n",
    "\n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    answer = a_encoder.decode(predicted_tokens)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "970ef50b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력:  이봐요\n",
      "출력: 좋은 찾아보세요\n"
     ]
    }
   ],
   "source": [
    "# 예시 입력 문장\n",
    "input_sentence = \" 이봐요\"\n",
    "predicted_answer = predict_answer(input_sentence, transformer_model, q_encoder, a_encoder, maxlen)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"입력: {input_sentence}\")\n",
    "print(f\"출력: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 모델을 인코더, 디코더 구조로 해서 다시 시도\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a812eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding_1  (None, 40, 128)      1108096     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_4 (Transforme (None, 40, 128)      396032      token_and_position_embedding_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_5 (Transforme (None, 40, 128)      396032      transformer_block_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_6 (Transforme (None, 40, 128)      396032      transformer_block_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_7 (Transforme (None, 40, 128)      396032      transformer_block_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Functional)            (None, 40, 8617)     4860073     input_7[0][0]                    \n",
      "                                                                 transformer_block_7[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 7,552,297\n",
      "Trainable params: 7,552,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 인코더 정의\n",
    "def build_encoder(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, maxlen):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# 디코더 정의\n",
    "def build_decoder(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, maxlen):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    encoder_outputs = Input(shape=(maxlen, embed_dim))\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        # 디코더는 인코더 출력과 자기 자신에 대해 어텐션 수행\n",
    "        attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        attn1 = Dropout(0.1)(attn1)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(x + attn1)\n",
    "\n",
    "        attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(out1, encoder_outputs)\n",
    "        attn2 = Dropout(0.1)(attn2)\n",
    "        out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "        ffn_output = Dense(ff_dim, activation=\"relu\")(out2)\n",
    "        ffn_output = Dense(embed_dim)(ffn_output)\n",
    "        x = LayerNormalization(epsilon=1e-6)(out2 + ffn_output)\n",
    "\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs=[inputs, encoder_outputs], outputs=outputs)\n",
    "\n",
    "# 인코더-디코더 모델 통합\n",
    "def build_encoder_decoder_model(vocab_size, embed_dim=128, num_heads=4, ff_dim=512, num_blocks=4, maxlen=40):\n",
    "    encoder = build_encoder(vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                            num_heads=num_heads, ff_dim=ff_dim,\n",
    "                            num_blocks=num_blocks, maxlen=maxlen)\n",
    "    \n",
    "    decoder = build_decoder(vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                            num_heads=num_heads, ff_dim=ff_dim,\n",
    "                            num_blocks=num_blocks, maxlen=maxlen)\n",
    "    \n",
    "    encoder_inputs = encoder.input\n",
    "    decoder_inputs = decoder.input[0]\n",
    "    \n",
    "    # 디코더에 인코더 출력 연결\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder.output])\n",
    "    \n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "vocab_size = q_encoder.vocab_size  # 기존 SubwordTextEncoder에서 가져옴\n",
    "maxlen = 40  # 기존 설정 유지\n",
    "encoder_decoder_model = build_encoder_decoder_model(vocab_size=vocab_size)\n",
    "\n",
    "# 모델 요약 출력\n",
    "encoder_decoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26445f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 입력과 출력 데이터 생성\n",
    "start_token_id = a_encoder.encode('<start>')[0]\n",
    "end_token_id = a_encoder.encode('<end>')[0]\n",
    "\n",
    "decoder_input_data = [[start_token_id] + seq for seq in a_tokenized]\n",
    "decoder_output_data = [seq + [end_token_id] for seq in a_tokenized]\n",
    "\n",
    "# 패딩 처리\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=maxlen, padding='post')\n",
    "decoder_output_data = pad_sequences(decoder_output_data, maxlen=maxlen, padding='post')\n",
    "\n",
    "# 학습 데이터 분리 (기존 질문 데이터도 사용)\n",
    "q_train_enc_inp, q_test_enc_inp, a_train_dec_inp, a_test_dec_inp,\\\n",
    "a_train_dec_outp, a_test_dec_outp = train_test_split(\n",
    "    q_data,\n",
    "    decoder_input_data,\n",
    "    decoder_output_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b4ec097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "148/148 [==============================] - 21s 92ms/step - loss: 2.0622 - accuracy: 0.8607 - val_loss: 0.9545 - val_accuracy: 0.8945\n",
      "Epoch 2/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.8676 - accuracy: 0.8958 - val_loss: 0.8601 - val_accuracy: 0.8962\n",
      "Epoch 3/10\n",
      "148/148 [==============================] - 13s 87ms/step - loss: 0.7652 - accuracy: 0.9010 - val_loss: 0.7148 - val_accuracy: 0.9056\n",
      "Epoch 4/10\n",
      "148/148 [==============================] - 13s 87ms/step - loss: 0.6010 - accuracy: 0.9154 - val_loss: 0.5591 - val_accuracy: 0.9247\n",
      "Epoch 5/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.4154 - accuracy: 0.9424 - val_loss: 0.4049 - val_accuracy: 0.9479\n",
      "Epoch 6/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.2624 - accuracy: 0.9672 - val_loss: 0.3001 - val_accuracy: 0.9651\n",
      "Epoch 7/10\n",
      "148/148 [==============================] - 13s 85ms/step - loss: 0.1565 - accuracy: 0.9846 - val_loss: 0.2305 - val_accuracy: 0.9756\n",
      "Epoch 8/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.0862 - accuracy: 0.9947 - val_loss: 0.1820 - val_accuracy: 0.9813\n",
      "Epoch 9/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.0437 - accuracy: 0.9983 - val_loss: 0.1486 - val_accuracy: 0.9854\n",
      "Epoch 10/10\n",
      "148/148 [==============================] - 13s 86ms/step - loss: 0.0210 - accuracy: 0.9996 - val_loss: 0.1272 - val_accuracy: 0.9872\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 0.1272 - accuracy: 0.9872\n",
      "테스트 손실: 0.1271609514951706\n",
      "테스트 정확도: 0.9871721863746643\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "encoder_decoder_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                              loss='sparse_categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "history = encoder_decoder_model.fit(\n",
    "    [q_train_enc_inp, a_train_dec_inp],\n",
    "    a_train_dec_outp,\n",
    "    validation_data=([q_test_enc_inp, a_test_dec_inp], a_test_dec_outp),\n",
    "    batch_size=64,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 테스트 평가\n",
    "test_loss, test_accuracy = encoder_decoder_model.evaluate(\n",
    "    [q_test_enc_inp, a_test_dec_inp], a_test_dec_outp)\n",
    "print(f\"테스트 손실: {test_loss}\")\n",
    "print(f\"테스트 정확도: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4740cf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 이봐요\n",
      "출력: 제가 치과에 \n"
     ]
    }
   ],
   "source": [
    "# 예시 입력 문장\n",
    "input_sentence = \"이봐요\"\n",
    "\n",
    "# 모델을 사용하여 답변 생성\n",
    "predicted_answer = predict_answer(input_sentence, transformer_model, q_encoder, a_encoder, maxlen)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"입력: {input_sentence}\")\n",
    "print(f\"출력: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033cf6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
